{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Let us do sample inferences by connecting to the public swarm, hosted on petals swarm.\n"
      ],
      "metadata": {
        "id": "IShdPlWWf532"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing the dependencies\n"
      ],
      "metadata": {
        "id": "5qY1naN5dNNN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHAxkxVc2098"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/bigscience-workshop/petals"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## To connect to Public Swarm, we specify the model name here"
      ],
      "metadata": {
        "id": "PA6cEQCidUPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from petals import AutoDistributedModelForCausalLM\n",
        "\n",
        "model_name = \"petals-team/StableBeluga2\"\n",
        "# You can also use any other supported model from ðŸ¤— Model Hub\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, add_bos_token=False)\n",
        "model = AutoDistributedModelForCausalLM.from_pretrained(model_name)\n",
        "model = model.cuda()"
      ],
      "metadata": {
        "id": "lhYE3IwY2_Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We do a simple inference after connecting to the swarm\n",
        "\n",
        "Let's try to generate something by calling __`model.generate()`__ method.\n",
        "\n",
        "The first call to this method takes a few seconds to connect to the Petals swarm. Once we do that, you should expect generation speed of up to **5-6 tokens/sec**. If you don't have enough GPU memory to host the entire model, this is much faster than what you get with other methods, such as offloading or running the model on CPU.\n"
      ],
      "metadata": {
        "id": "gtx0d-vEdeiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer('A cat in French is \"', return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "outputs = model.generate(inputs, max_new_tokens=3)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "aa8CAAKydknr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's make a chatbot\n",
        "\n",
        "If you'd like to talk to the model in an interactive way, you can use the inference session interface â€” it allows to print generated tokens on the fly or make a chat bot that responds to human's inputs.\n",
        "\n",
        "The inference session looks for a sequence of servers to run successive inference steps and store past attention caches. This way, you don't need to rerun previous tokens through the transformer to generate each phrase. If one of the servers disconnects or fails, Petals will automatically find a replacement and regenerate only a small part of the caches."
      ],
      "metadata": {
        "id": "bYCCVV-eeV_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with model.inference_session(max_length=512) as sess:\n",
        "    while True:\n",
        "        prompt = input('Human: ')\n",
        "        if prompt == \"\":\n",
        "            break\n",
        "        prefix = f\"Human: {prompt}\\nFriendly AI:\"\n",
        "        prefix = tokenizer(prefix, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
        "        print(\"Friendly AI:\", end=\"\", flush=True)\n",
        "\n",
        "        while True:\n",
        "            outputs = model.generate(prefix, max_new_tokens=1, session=sess,\n",
        "                                     do_sample=True, temperature=0.9, top_p=0.6)\n",
        "            outputs = tokenizer.decode([fake_token, outputs[0, -1].item()])[1:]\n",
        "\n",
        "            # Now, let's print one new token at a time\n",
        "            print(outputs, end=\"\", flush=True)\n",
        "\n",
        "            if \"\\n\" in outputs or \"</s>\" in outputs:\n",
        "                break\n",
        "            prefix = None  # Prefix is passed only for the 1st token of the bot's response"
      ],
      "metadata": {
        "id": "HmiwHTLfepKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does it work?\n",
        "The model you are running is equal to the original model, but only a part of it is loaded into your machine's GPU. Let's have a look under the hood:"
      ],
      "metadata": {
        "id": "wJasjakTevwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "Xc0HQqije7FW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, word embeddings and some other layers are regular PyTorch modules hosted on your machine, but the rest of the model (e.g., transformers blocks) is encased in the RemoteSequential class. This is an advanced PyTorch module that runs on a distributed swarm of other machines.\n",
        "\n",
        "Still, you can access individual layers and their outputs, as well as run forward/backward through them:\n",
        "\n"
      ],
      "metadata": {
        "id": "M2q0miibe_nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now let us run a benchmark test like MMLU Test to check the quality of our model"
      ],
      "metadata": {
        "id": "awYhecKjfPfw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the helper functions for the benchmarking test"
      ],
      "metadata": {
        "id": "-ojIeSOAfhkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "import tensor_parallel as tp\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "TASKS = [\n",
        "        'global_facts',\n",
        "        'high_school_computer_science',\n",
        "        'high_school_mathematics',\n",
        "        'high_school_physics',\n",
        "        'machine_learning',\n",
        "        'miscellaneous',\n",
        "        'moral_disputes']\n",
        "\n",
        "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
        "\n",
        "def compute_metric(output_filename):\n",
        "    with open(output_filename, 'r') as f:\n",
        "        run_results = json.load(f)\n",
        "    total_acc = 0\n",
        "    total_num = 0\n",
        "    for task in run_results:\n",
        "        acc = 0\n",
        "        pred_answers = run_results[task]['pred_answers']\n",
        "        gold_answers = run_results[task]['gold_answers']\n",
        "        for pred, gold in zip(pred_answers, gold_answers):\n",
        "            if pred == gold: acc += 1\n",
        "        print(\"ACC-%s: %.4f\" % (task, acc/len(gold_answers)))\n",
        "        total_acc += acc\n",
        "        total_num += len(gold_answers)\n",
        "    print(\"ACC-all: %.4f\" % (total_acc/total_num))\n",
        "\n",
        "\n",
        "def format_subject(subject):\n",
        "    l = subject.split(\"_\")\n",
        "    s = \"\"\n",
        "    for entry in l:\n",
        "        s += \" \" + entry\n",
        "    return s\n",
        "\n",
        "def format_example(df, idx, include_answer=True):\n",
        "    prompt = df.iloc[idx, 0]\n",
        "    k = df.shape[1] - 2\n",
        "    for j in range(k):\n",
        "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j+1])\n",
        "    prompt += \"\\nAnswer:\"\n",
        "    if include_answer:\n",
        "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
        "    return prompt\n",
        "\n",
        "def gen_prompt(train_df, subject, k=-1):\n",
        "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(format_subject(subject))\n",
        "    if k == -1:\n",
        "        k = train_df.shape[0]\n",
        "    for i in range(k):\n",
        "        prompt += format_example(train_df, i)\n",
        "    return prompt\n",
        "\n",
        "\n",
        "# def custom_stopping_criteria(input_ids, score, **kwargs):\n",
        "#     stop_ids = [29871, 13, 13] # \\n\\n\n",
        "#     return input_ids[-len(stop_ids)]\n",
        "\n",
        "\n",
        "def load(model_type):\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "\n",
        "    if model_type == 'llama':\n",
        "        # we use tensor parallel for loading llama\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, add_bos_token=False, use_auth_token=\"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\")\n",
        "\n",
        "        model = AutoDistributedModelForCausalLM.from_pretrained(model_name, use_auth_token=\"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\")\n",
        "        model = model.cuda()\n",
        "\n",
        "        tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
        "        tokenizer.bos_token_id = 1\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "\n",
        "def serial_infer(model, tokenizer, prompts):\n",
        "    answers = []\n",
        "    for prompt in tqdm(prompts):\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to('cuda')\n",
        "        output = model.generate(inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id, attention_mask=None)\n",
        "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        answers.append(answer[-1])\n",
        "    return answers\n"
      ],
      "metadata": {
        "id": "5R2x4M8AGE9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main function that will run the benchmark test\n"
      ],
      "metadata": {
        "id": "vcM_AJXjfpY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir= 'Cloud_Computing_Project/data'\n",
        "param_size='70B'\n",
        "model_type='llama'\n",
        "run_results = {}\n",
        "output_filename = 'run_results_%s_%sb.json' % (model_type, param_size)\n",
        "model, tokenizer = load(model_type)\n",
        "start_time = time.time()\n",
        "\n",
        "for task in TASKS:\n",
        "    print('Testing %s ...' % task)\n",
        "    records = []\n",
        "    dev_df = pd.read_csv(os.path.join(data_dir, \"dev\", task + \"_dev.csv\"), header=None)[:5]\n",
        "    test_df = pd.read_csv(os.path.join(data_dir, \"test\", task + \"_test.csv\"), header=None)\n",
        "\n",
        "    for i in range(test_df.shape[0]):\n",
        "            # get prompt and make sure it fits\n",
        "        k = 5\n",
        "        prompt_end = format_example(test_df, i, include_answer=False)\n",
        "        train_prompt = gen_prompt(dev_df, task, k)\n",
        "        prompt = train_prompt + prompt_end\n",
        "        while len(tokenizer.tokenize(prompt)) + 1 > 2048:  # bos token\n",
        "            prompt_split = prompt.split(\"\\n\\n\")\n",
        "            prompt_split.pop(1)\n",
        "            prompt = '\\n\\n'.join(prompt_split)\n",
        "\n",
        "        label = test_df.iloc[i, test_df.shape[1] - 1]\n",
        "        records.append({'prompt': prompt, 'answer': label})\n",
        "\n",
        "        pred_answers = []\n",
        "        for record in records:\n",
        "            inputs = tokenizer(record['prompt'], return_tensors=\"pt\")[\"input_ids\"].to('cuda')\n",
        "            output = model.generate(inputs, max_new_tokens=1, pad_token_id=tokenizer.pad_token_id, attention_mask=None)\n",
        "            answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "            pred_answers.append(answer[-1])\n",
        "\n",
        "        gold_answers = [record['answer'] for record in records]\n",
        "        run_results[task] = {'pred_answers': pred_answers, 'gold_answers': gold_answers}"
      ],
      "metadata": {
        "id": "HD49VVRWH8Ct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Results can be seen here"
      ],
      "metadata": {
        "id": "RHuKAjsHfvKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(output_filename, 'w') as f:\n",
        "    json.dump(run_results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "compute_metric(output_filename)\n",
        "end_time = time.time()\n",
        "print(\"total run time %.2f\" % (end_time - start_time))"
      ],
      "metadata": {
        "id": "OpwPv9wuTVpt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}